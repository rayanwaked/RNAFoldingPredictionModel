{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":11228175,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stanford RNA 3D Folding Competition Notebook\n\nThis notebook is designed for the \"Stanford RNA 3D Folding\" Kaggle competition.\nIt covers:\n\n1. Data Exploration  \n2. Data Preprocessing  \n   - Sequence encoding  \n   - Label grouping and padding (with NaN handling)\n3. Model Building using a fast CNN architecture  \n4. Model Training with early stopping  \n5. Prediction on test set and submission file generation\n\n_Note: This notebook uses only the provided CSV files (no external internet access)._","metadata":{"_uuid":"ba75c96f-68e8-4eb7-8159-5d762f35366f","_cell_guid":"4cd7fda1-faf2-4318-8c84-a231a515b338","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1. Import Libraries","metadata":{"_uuid":"4974e6d3-499d-4e64-ab6f-eb6b1fd4ba99","_cell_guid":"2e35b03c-2c0b-49fe-945d-e632cd4e7eeb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# TensorFlow/Keras for deep learning model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, BatchNormalization, Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import losses\n\n# Ensure GPU usage\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Restrict to first GPU and set memory growth\n        tf.config.set_visible_devices(gpus[0], 'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        print(\"Using GPU:\", gpus[0])\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"Using CPU\")\n\n# Define pairs\ndef is_complementary(base1, base2):\n    \"\"\"Check if two nucleotides are complementary.\"\"\"\n    pairs = {\n        'A': ['U'],  # Adenine pairs with Uracil\n        'U': ['A', 'G'],  # Uracil pairs with Adenine or Guanine\n        'G': ['C', 'U'],  # Guanine pairs with Cytosine or Uracil\n        'C': ['G']  # Cytosine pairs with Guanine\n    }\n    return base2 in pairs.get(base1, [])\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"_uuid":"1c8a6414-5cce-4cf9-82dc-74c2b1937448","_cell_guid":"95be3c1b-d91d-4ff2-aeb1-16b50fa1936e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:00.640518Z","iopub.execute_input":"2025-02-28T00:37:00.640828Z","iopub.status.idle":"2025-02-28T00:37:00.662255Z","shell.execute_reply.started":"2025-02-28T00:37:00.640804Z","shell.execute_reply":"2025-02-28T00:37:00.661397Z"}},"outputs":[{"name":"stdout","text":"Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## 2. Data Loading and Exploration\n\nWe load the CSV files provided in the competition:\n- `train_sequences.csv`\n- `train_labels.csv`\n- `validation_sequences.csv` & `validation_labels.csv`\n- `test_sequences.csv`\n- `sample_submission.csv`\n\n**Important:** We fill missing values in the labels data with 0 to avoid NaN issues during training.","metadata":{"_uuid":"7d1c7eed-82d4-46ea-859c-d7a7490d9852","_cell_guid":"08036a25-432d-4eca-bbfe-961e87c3f83a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define file paths (Kaggle input paths)\nTRAIN_SEQ_PATH = '/kaggle/input/stanford-rna-3d-folding/train_sequences.csv'\nTRAIN_LABELS_PATH = '/kaggle/input/stanford-rna-3d-folding/train_labels.csv'\nVALID_SEQ_PATH = '/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv'\nVALID_LABELS_PATH = '/kaggle/input/stanford-rna-3d-folding/validation_labels.csv'\nTEST_SEQ_PATH  = '/kaggle/input/stanford-rna-3d-folding/test_sequences.csv'\nSAMPLE_SUB_PATH = '/kaggle/input/stanford-rna-3d-folding/sample_submission.csv'\n\n# Load CSV files\ntrain_sequences = pd.read_csv(TRAIN_SEQ_PATH)\ntrain_labels = pd.read_csv(TRAIN_LABELS_PATH)\nvalid_sequences = pd.read_csv(VALID_SEQ_PATH)\nvalid_labels = pd.read_csv(VALID_LABELS_PATH)\ntest_sequences = pd.read_csv(TEST_SEQ_PATH)\nsample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n\n# Fill missing values in labels with 0\ntrain_labels.fillna(0, inplace=True)\nvalid_labels.fillna(0, inplace=True)\n\n# Display basic info\nprint(\"Train Sequences Shape:\", train_sequences.shape)\nprint(\"Train Labels Shape:\", train_labels.shape)\nprint(\"Validation Sequences Shape:\", valid_sequences.shape)\nprint(\"Validation Labels Shape:\", valid_labels.shape)\nprint(\"Test Sequences Shape:\", test_sequences.shape)\n\n# Look at a few examples\nprint(\"\\nTrain Sequences Head:\")\nprint(train_sequences.head())\nprint(\"\\nTrain Labels Head:\")\nprint(train_labels.head())","metadata":{"_uuid":"bd1b8c77-f907-4a3b-af02-5141a285d2ac","_cell_guid":"9f130e5f-7f0d-424f-8634-f304c96c9437","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:00.665315Z","iopub.execute_input":"2025-02-28T00:37:00.665542Z","iopub.status.idle":"2025-02-28T00:37:00.950132Z","shell.execute_reply.started":"2025-02-28T00:37:00.665523Z","shell.execute_reply":"2025-02-28T00:37:00.949414Z"}},"outputs":[{"name":"stdout","text":"Train Sequences Shape: (844, 5)\nTrain Labels Shape: (137095, 6)\nValidation Sequences Shape: (12, 5)\nValidation Labels Shape: (2515, 123)\nTest Sequences Shape: (12, 5)\n\nTrain Sequences Head:\n  target_id                            sequence temporal_cutoff  \\\n0    1SCL_A       GGGUGCUCAGUACGAGAGGAACCGCACCC      1995-01-26   \n1    1RNK_A  GGCGCAGUGGGCUAGCGCCACUCAAAAGGCCCAU      1995-02-27   \n2    1RHT_A            GGGACUGACGAUCACGCAGUCUAU      1995-06-03   \n3    1HLX_A                GGGAUAACUUCGGUUGUCCC      1995-09-15   \n4    1HMH_E  GGCGACCCUGAUGAGGCCGAAAGGCCGAAACCGU      1995-12-07   \n\n                                         description  \\\n0               THE SARCIN-RICIN LOOP, A MODULAR RNA   \n1  THE STRUCTURE OF AN RNA PSEUDOKNOT THAT CAUSES...   \n2  24-MER RNA HAIRPIN COAT PROTEIN BINDING SITE F...   \n3  P1 HELIX NUCLEIC ACIDS (DNA/RNA) RIBONUCLEIC ACID   \n4  THREE-DIMENSIONAL STRUCTURE OF A HAMMERHEAD RI...   \n\n                                       all_sequences  \n0  >1SCL_1|Chain A|RNA SARCIN-RICIN LOOP|Rattus n...  \n1  >1RNK_1|Chain A|RNA PSEUDOKNOT|null\\nGGCGCAGUG...  \n2  >1RHT_1|Chain A|RNA (5'-R(P*GP*GP*GP*AP*CP*UP*...  \n3  >1HLX_1|Chain A|RNA (5'-R(*GP*GP*GP*AP*UP*AP*A...  \n4  >1HMH_1|Chains A, C, E|HAMMERHEAD RIBOZYME-RNA...  \n\nTrain Labels Head:\n         ID resname  resid     x_1        y_1     z_1\n0  1SCL_A_1       G      1  13.760 -25.974001   0.102\n1  1SCL_A_2       G      2   9.310 -29.638000   2.669\n2  1SCL_A_3       G      3   5.529 -27.813000   5.878\n3  1SCL_A_4       U      4   2.678 -24.900999   9.793\n4  1SCL_A_5       G      5   1.827 -20.136000  11.793\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## 3. Data Preprocessing\n\n### 3.1 Sequence Encoding\n\nWe map each nucleotide to an integer:\n- A: 1, C: 2, G: 3, U: 4  \nUnknown characters are mapped to 0.","metadata":{"_uuid":"7b3ae69d-4427-4c9f-931e-9ea1176f5886","_cell_guid":"f84ddf2c-b665-420f-a7ec-43cfe711c0a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from collections import defaultdict\n\nnucleotide_map = {'A': 1, 'C': 2, 'G': 3, 'U': 4}\n\ndef encode_sequence(seq, known_sequences):\n    \"\"\"Encodes an RNA sequence into a list of integers, replacing unknown nucleotides\n       with the most similar known nucleotide's mapping.\n    \"\"\"\n    encoded_seq = []\n    for ch in seq:\n        if ch in nucleotide_map:\n            encoded_seq.append(nucleotide_map[ch])\n        else:\n            # Find the most similar known nucleotide\n            most_similar = find_most_similar_nucleotide(ch, known_sequences)\n            if most_similar:\n                encoded_seq.append(nucleotide_map[most_similar])\n            else:\n                # If no similar nucleotide is found, handle accordingly (e.g., raise an error, use a default value, etc.)\n                encoded_seq.append(0)  # Or raise ValueError(f\"Unknown nucleotide: {ch}\")\n\n    return encoded_seq\n\ndef find_most_similar_nucleotide(unknown_nucleotide, known_sequences):\n    \"\"\"Finds the most similar nucleotide from known sequences.\"\"\"\n    similarity_counts = defaultdict(int)\n\n    for known_seq in known_sequences:\n        for known_ch in known_seq:\n            if known_ch in nucleotide_map:\n                if are_similar(unknown_nucleotide, known_ch):\n                    similarity_counts[known_ch] += 1\n\n    if similarity_counts:\n        return max(similarity_counts, key=similarity_counts.get)\n    else:\n        return None\n\ndef are_similar(unknown_nucleotide, known_nucleotide):\n    \"\"\"Determines if two nucleotides are similar.\n       This is a placeholder; you'll need to define your similarity logic.\n       Example: considering 'N' as any nucleotide.\n    \"\"\"\n    if unknown_nucleotide == known_nucleotide:\n        return True\n    if unknown_nucleotide == 'N': # N means any nucleotide\n        return True\n    if known_nucleotide == 'N': # N means any nucleotide.\n        return True\n    # Add other similarity rules as needed.\n    return False\n\ndef preprocess_sequences(sequences_df, known_sequences):\n    \"\"\"Encodes sequences in a DataFrame, using known sequences for unknown nucleotides.\"\"\"\n    sequences_df['encoded'] = sequences_df['sequence'].apply(lambda seq: encode_sequence(seq, known_sequences))\n    return sequences_df\n\n# Assuming train_sequences, valid_sequences, and test_sequences are pandas DataFrames\n# and 'sequence' is the column containing the RNA sequences.\n\n# Create a list of all known nucleotides from your training set.\nknown_sequences = train_sequences['sequence'].tolist()\n\n# Apply encoding to all sequence files\ntrain_sequences = preprocess_sequences(train_sequences, known_sequences)\nvalid_sequences = preprocess_sequences(valid_sequences, known_sequences)\ntest_sequences = preprocess_sequences(test_sequences, known_sequences)","metadata":{"_uuid":"2f38e291-5974-4c43-93a0-fb024c343ba0","_cell_guid":"3c95facf-6580-4e0d-bf92-9cfb9539a551","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:00.951492Z","iopub.execute_input":"2025-02-28T00:37:00.951715Z","iopub.status.idle":"2025-02-28T00:37:01.147638Z","shell.execute_reply.started":"2025-02-28T00:37:00.951695Z","shell.execute_reply":"2025-02-28T00:37:01.146712Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"### 3.2 Processing Label Data\n\nEach row in the labels CSV is for one residue, with an `ID` formatted as `target_id_resid`.\nWe group rows by `target_id` and sort by residue number.\nHere, we use the first structure (x_1, y_1, z_1) as our target coordinates.","metadata":{"_uuid":"fbc2c63d-cb94-493c-9d4f-e0cf078d07c1","_cell_guid":"c3189e08-66bf-4548-8f33-3422b765f88c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def process_labels(labels_df):\n    \"\"\"\n    Processes a labels DataFrame by grouping rows by target_id.\n    Returns a dictionary mapping target_id to an array of coordinates (seq_len, 3).\n    \"\"\"\n    label_dict = {}\n    for idx, row in labels_df.iterrows():\n        # Split ID into target_id and residue number (assumes format \"targetid_resid\")\n        parts = row['ID'].split('_')\n        target_id = \"_\".join(parts[:-1])\n        resid = int(parts[-1])\n        # Extract the coordinates; they should be numeric (missing values already set to 0)\n        coord = np.array([row['x_1'], row['y_1'], row['z_1']], dtype=np.float32)\n        if target_id not in label_dict:\n            label_dict[target_id] = []\n        label_dict[target_id].append((resid, coord))\n    \n    # Sort residues by resid and stack coordinates\n    for key in label_dict:\n        sorted_coords = sorted(label_dict[key], key=lambda x: x[0])\n        coords = np.stack([c for r, c in sorted_coords])\n        label_dict[key] = coords\n    return label_dict\n\n# Process training and validation labels\ntrain_labels_dict = process_labels(train_labels)\nvalid_labels_dict = process_labels(valid_labels)","metadata":{"_uuid":"1d94dbc2-d120-4c82-9d70-711e6b4a68c1","_cell_guid":"afbd9f74-c685-4e0a-9262-005656f79ca9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:01.149128Z","iopub.execute_input":"2025-02-28T00:37:01.149429Z","iopub.status.idle":"2025-02-28T00:37:08.091908Z","shell.execute_reply.started":"2025-02-28T00:37:01.149401Z","shell.execute_reply":"2025-02-28T00:37:08.091172Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"### 3.3 Creating Datasets and Padding\n\nWe match each target sequence with its corresponding coordinate labels.\nThen we pad sequences and coordinate arrays to a uniform length.\n\nPadded positions in coordinates are set to 0.","metadata":{"_uuid":"acfc3ec8-97ac-44cf-a2dc-d1a721b1c8d8","_cell_guid":"1508c42f-9b88-4934-be7d-c75fcc36195c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def create_dataset(sequences_df, labels_dict):\n    \"\"\"\n    Creates a dataset from a sequences DataFrame and a labels dictionary.\n    Returns:\n        X: list of encoded sequences,\n        y: list of coordinate arrays,\n        target_ids: list of target ids.\n    \"\"\"\n    X, y, target_ids = [], [], []\n    for idx, row in sequences_df.iterrows():\n        tid = row['target_id']\n        if tid in labels_dict:\n            X.append(row['encoded'])\n            y.append(labels_dict[tid])\n            target_ids.append(tid)\n    return X, y, target_ids\n\n# Create training and validation datasets\nX_train, y_train, train_ids = create_dataset(train_sequences, train_labels_dict)\nX_valid, y_valid, valid_ids = create_dataset(valid_sequences, valid_labels_dict)\n\n# Determine maximum sequence length from training set\nmax_len = max(len(seq) for seq in X_train)\nprint(\"Maximum sequence length (train):\", max_len)\n\n# Pad the sequences (padding value = 0)\nX_train_pad = pad_sequences(X_train, maxlen=max_len, padding='post', value=0)\nX_valid_pad = pad_sequences(X_valid, maxlen=max_len, padding='post', value=0)\n\n# Function to pad coordinate arrays\ndef pad_coordinates(coord_array, max_len):\n    L = coord_array.shape[0]\n    if L < max_len:\n        pad_width = ((0, max_len - L), (0, 0))\n        return np.pad(coord_array, pad_width, mode='constant', constant_values=0)\n    else:\n        return coord_array\n\n# Pad coordinate arrays\ny_train_pad = np.array([pad_coordinates(arr, max_len) for arr in y_train])\ny_valid_pad = np.array([pad_coordinates(arr, max_len) for arr in y_valid])\n\n# Check for any NaN values in the targets\nprint(\"Any NaN in y_train_pad?\", np.isnan(y_train_pad).any())\nprint(\"Any NaN in y_valid_pad?\", np.isnan(y_valid_pad).any())\n\nprint(\"X_train_pad shape:\", X_train_pad.shape)\nprint(\"y_train_pad shape:\", y_train_pad.shape)","metadata":{"_uuid":"f00ec075-2695-461e-a423-b8b251c43acd","_cell_guid":"be76be8c-682f-418d-94d0-4f6bb519b344","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:08.092722Z","iopub.execute_input":"2025-02-28T00:37:08.093050Z","iopub.status.idle":"2025-02-28T00:37:08.219267Z","shell.execute_reply.started":"2025-02-28T00:37:08.093021Z","shell.execute_reply":"2025-02-28T00:37:08.218333Z"}},"outputs":[{"name":"stdout","text":"Maximum sequence length (train): 4298\nAny NaN in y_train_pad? False\nAny NaN in y_valid_pad? False\nX_train_pad shape: (844, 4298)\ny_train_pad shape: (844, 4298, 3)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 4. Fast CNN Model Training\n\nIn this section, we build a faster CNN-based model.\nThe model uses:\n- An Embedding layer  \n- Two Conv1D blocks (with BatchNormalization and Dropout)  \n- A final Conv1D layer (kernel size 1) to output 3 coordinates per residue","metadata":{"_uuid":"8d832ed7-a808-47b1-9bd1-4ca8aa72b337","_cell_guid":"9616835f-b774-4de4-bb7d-99fffe07dc65","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define hyperparameters for the CNN model\nvocab_size = max(nucleotide_map.values()) + 1  # +1 for padding token 0\nembedding_dim = 16\nnum_filters = 64\nkernel_size = 3\ndrop_rate = 0.2\n\n# Build the CNN model\ninput_seq_cnn = Input(shape=(max_len,), name='input_seq')\nx_cnn = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, name='embedding')(input_seq_cnn)\n\n# First convolutional block\nx_cnn = Conv1D(filters=num_filters, kernel_size=kernel_size, padding='same', activation='relu', name='conv1')(x_cnn)\nx_cnn = BatchNormalization(name='bn1')(x_cnn)\nx_cnn = Dropout(drop_rate, name='drop1')(x_cnn)\n\n# Second convolutional block\nx_cnn = Conv1D(filters=num_filters, kernel_size=kernel_size, padding='same', activation='relu', name='conv2')(x_cnn)\nx_cnn = BatchNormalization(name='bn2')(x_cnn)\nx_cnn = Dropout(drop_rate, name='drop2')(x_cnn)\n\n# Final convolution to output 3 coordinates per residue (x, y, z)\noutput_coords_cnn = Conv1D(filters=3, kernel_size=1, padding='same', activation='linear', name='predicted_coords')(x_cnn)\n\ncnn_model = Model(inputs=input_seq_cnn, outputs=output_coords_cnn)\ncnn_model.compile(optimizer='adam', loss='mse')\n\ncnn_model.summary()","metadata":{"_uuid":"b5b77c10-dad8-4527-8f28-174c04afefe2","_cell_guid":"eb0e2b5f-347a-45b9-a475-a2a4108dd9f1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:08.220228Z","iopub.execute_input":"2025-02-28T00:37:08.220559Z","iopub.status.idle":"2025-02-28T00:37:08.308729Z","shell.execute_reply.started":"2025-02-28T00:37:08.220523Z","shell.execute_reply":"2025-02-28T00:37:08.308041Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'conv1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_11\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_seq (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m16\u001b[0m)            │              \u001b[38;5;34m80\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1 (\u001b[38;5;33mConv1D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │           \u001b[38;5;34m3,136\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ drop1 (\u001b[38;5;33mDropout\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2 (\u001b[38;5;33mConv1D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m12,352\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ drop2 (\u001b[38;5;33mDropout\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ predicted_coords (\u001b[38;5;33mConv1D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4298\u001b[0m, \u001b[38;5;34m3\u001b[0m)             │             \u001b[38;5;34m195\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_seq (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ drop1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ drop2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ predicted_coords (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,275\u001b[0m (63.57 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,275</span> (63.57 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,019\u001b[0m (62.57 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,019</span> (62.57 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n</pre>\n"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"## 5. Model Training\n\nWe train the CNN model using early stopping to monitor the validation loss.\nWith the NaN issues addressed in the data, training should proceed without nan losses.","metadata":{"_uuid":"93c354e3-05d7-4632-89cb-b9c843c37977","_cell_guid":"7744df17-746a-438d-82eb-3fda8874d03c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# 5. Model Training (Original Teacher)\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_pad, y_train_pad))\ntrain_dataset = train_dataset.shuffle(1000).batch(64).prefetch(tf.data.AUTOTUNE)\n\nvalid_dataset = tf.data.Dataset.from_tensor_slices((X_valid_pad, y_valid_pad))\nvalid_dataset = valid_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n\nearly_stop_cnn = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nhistory_cnn = cnn_model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=50,\n    callbacks=[early_stop_cnn],\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T00:37:08.309507Z","iopub.execute_input":"2025-02-28T00:37:08.309771Z","iopub.status.idle":"2025-02-28T00:37:16.087166Z","shell.execute_reply.started":"2025-02-28T00:37:08.309740Z","shell.execute_reply":"2025-02-28T00:37:16.086326Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 189ms/step - loss: 573.8710 - val_loss: 290832909220992902620675565420544.0000\nEpoch 2/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 971.4595 - val_loss: 290832909220992902620675565420544.0000\nEpoch 3/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 615.1906 - val_loss: 290832909220992902620675565420544.0000\nEpoch 4/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 834.8428 - val_loss: 290832909220992902620675565420544.0000\nEpoch 5/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 567.2512 - val_loss: 290832909220992902620675565420544.0000\nEpoch 6/50\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 524.1871 - val_loss: 290832909220992902620675565420544.0000\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Embedding, Conv1D, BatchNormalization, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\n# Function to build the CNN model\ndef build_cnn_model():\n    input_seq = Input(shape=(max_len,), name='input_seq')\n    x = Embedding(vocab_size, embedding_dim, mask_zero=True, name='embedding')(input_seq)\n    x = Conv1D(num_filters, kernel_size, padding='same', activation='relu', name='conv1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = Dropout(drop_rate, name='drop1')(x)\n    x = Conv1D(num_filters, kernel_size, padding='same', activation='relu', name='conv2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    x = Dropout(drop_rate, name='drop2')(x)\n    outputs = Conv1D(3, 1, padding='same', activation='linear', name='predicted_coords')(x)\n    model = Model(inputs=input_seq, outputs=outputs)\n    return model\n\n# Define distillation loss combining true labels and teacher predictions\ndef combined_loss(y_true_combined, y_pred):\n    y_true = y_true_combined[..., :3]  # Original labels\n    y_teacher = y_true_combined[..., 3:]  # Teacher predictions\n    mse_true = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n    mse_teacher = tf.keras.losses.MeanSquaredError()(y_teacher, y_pred)\n    return 0.5 * mse_true + 0.5 * mse_teacher\n\n# Perform 3 distillation steps\nteacher_model = cnn_model  # Start with original model\nfor distil_step in range(3):\n    print(f\"\\nPerforming distillation step {distil_step + 1}/3\")\n\n    # Generate teacher predictions\n    print(\"Generating teacher predictions for training and validation data...\")\n    train_teacher_pred = teacher_model.predict(X_train_pad, verbose=1)\n    valid_teacher_pred = teacher_model.predict(X_valid_pad, verbose=1)\n\n    # Debug: Check shapes before concatenation\n    print(f\"y_train_pad shape: {y_train_pad.shape}\")\n    print(f\"train_teacher_pred shape: {train_teacher_pred.shape}\")\n\n    # Concatenate along the last axis\n    combined_y_train = np.concatenate([y_train_pad, train_teacher_pred], axis=-1)\n    combined_y_valid = np.concatenate([y_valid_pad, valid_teacher_pred], axis=-1)\n\n    # Debug: Check shapes after concatenation\n    print(f\"combined_y_train shape: {combined_y_train.shape}\")\n    print(f\"combined_y_valid shape: {combined_y_valid.shape}\")\n\n    # Create datasets for student training\n    student_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_pad, combined_y_train))\n    student_train_dataset = student_train_dataset.shuffle(1000).batch(64).prefetch(tf.data.AUTOTUNE)\n\n    # Create validation dataset\n    valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid_pad, combined_y_valid))\n    valid_dataset = valid_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n\n    # Build and train student model\n    student_model = build_cnn_model()\n    student_model.compile(optimizer='adam', loss=combined_loss)\n\n    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    print(\"Training student model...\")\n    history_student_model = student_model.fit(\n        student_train_dataset,\n        validation_data=valid_dataset,\n        epochs=50,\n        callbacks=[early_stop],\n        verbose=1\n    )\n\n    teacher_model = student_model  # Student becomes teacher for next step\n\n\n# Modified distillation loop with physical constraints\nteacher_model = cnn_model\nfor distil_step in range(3):\n    print(f\"\\nPerforming distillation step {distil_step + 1}/3 with physical constraints\")\n\n    # Generate teacher predictions with physical validation\n    print(\"Generating teacher predictions for training and validation data...\")\n    train_teacher_pred = teacher_model.predict(X_train_pad, verbose=0)\n    valid_teacher_pred = teacher_model.predict(X_valid_pad, verbose=0)\n\n    # Apply physical constraints to teacher predictions\n    def apply_physical_constraints(sequences, coords):\n        print(f\"Applying physical constraints to predictions...\")\n        constrained_coords = []\n        for seq, coord in zip(sequences, coords):\n            seq_str = seq['sequence']  # Extract the RNA sequence\n            valid_coords = tf.Variable(coord.copy())  # Create a TensorFlow variable for GPU computation\n\n            n = len(seq_str)\n            for i in range(n):\n                for j in range(i+4, n):  # Minimum loop length of 4\n                    if is_complementary(seq_str[i], seq_str[j]):\n                        # Compute distance using TensorFlow\n                        current_dist = tf.norm(valid_coords[i] - valid_coords[j])\n                        if current_dist < 2.8 or current_dist > 3.8:\n                            direction = (valid_coords[j] - valid_coords[i]) / (current_dist + 1e-8)\n                            valid_coords_i_updated = valid_coords[i] + direction * (3.4 - current_dist) * 0.1\n                            valid_coords_j_updated = valid_coords[j] - direction * (3.4 - current_dist) * 0.1\n                            valid_coords = tf.tensor_scatter_nd_update(valid_coords, [[i], [j]], [valid_coords_i_updated, valid_coords_j_updated])\n\n            # Resolve steric clashes\n            for i in range(n):\n                for j in range(i+2, n):  # Skip adjacent residues\n                    if abs(i-j) < 4: continue\n                    dist = tf.norm(valid_coords[i] - valid_coords[j])\n                    if dist < 2.0:\n                        direction = (valid_coords[j] - valid_coords[i]) / (dist + 1e-8)\n                        push_force = (2.0 - dist) * 0.2\n                        valid_coords_i_updated = valid_coords[i] - direction * push_force\n                        valid_coords_j_updated = valid_coords[j] + direction * push_force\n                        valid_coords = tf.tensor_scatter_nd_update(valid_coords, [[i], [j]], [valid_coords_i_updated, valid_coords_j_updated])\n\n            constrained_coords.append(valid_coords)\n        return tf.stack(constrained_coords)\n\n    # Apply constraints to teacher predictions\n    print(\"Applying physical constraints to teacher predictions...\")\n    train_teacher_pred = apply_physical_constraints(train_sequences.to_dict('records'), train_teacher_pred)\n    valid_teacher_pred = apply_physical_constraints(valid_sequences.to_dict('records'), valid_teacher_pred)\n\n    # Create enhanced training targets with physical guidance\n    combined_y_train = np.concatenate([y_train_pad, train_teacher_pred], axis=-1)\n    combined_y_valid = np.concatenate([y_valid_pad, valid_teacher_pred], axis=-1)\n\n    # Build student model with physics-aware loss\n    student_model = build_cnn_model()\n\n    # Enhanced loss function with physical regularization\n    def physics_aware_loss(y_true_combined, y_pred):\n        # Standard distillation loss\n        y_true = y_true_combined[..., :3]\n        y_teacher = y_true_combined[..., 3:]\n        mse_loss = 0.5 * tf.keras.losses.MeanSquaredError()(y_true, y_pred) + 0.5 * tf.keras.losses.MeanSquaredError()(y_teacher, y_pred)\n\n        # Physics regularization\n        batch_size = tf.shape(y_pred)[0]\n        phys_loss = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n\n        for i in tf.range(batch_size):\n            coords = y_pred[i]\n            seq = train_sequences.iloc[i.numpy()]['sequence']\n\n            # Calculate base pair satisfaction\n            pair_loss = 0.0\n            for j in range(len(seq)):\n                for k in range(j+4, len(seq)):\n                    if is_complementary(seq[j], seq[k]):\n                        dist = tf.norm(coords[j] - coords[k])\n                        pair_loss += tf.maximum(0.0, tf.abs(dist - 3.4) - 0.4)  # Allow 3.0-3.8Å range\n\n            # Calculate steric clash penalty\n            clash_loss = 0.0\n            for j in range(len(seq)):\n                for k in range(j+2, len(seq)):\n                    if abs(j-k) > 3:\n                        dist = tf.norm(coords[j] - coords[k])\n                        clash_loss += tf.maximum(0.0, 2.0 - dist)  # Penalize < 2.0Å\n\n            phys_loss = phys_loss.write(i, pair_loss + clash_loss)\n\n        total_phys_loss = tf.reduce_mean(phys_loss.stack())\n        return mse_loss + 0.1 * total_phys_loss  # Weighted combination\n\n    student_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001*0.8**distil_step),\n                          loss=physics_aware_loss)\n\n    # Training with physical validation callback\n    class PhysicalConstraintCallback(tf.keras.callbacks.Callback):\n        def on_epoch_end(self, epoch, logs=None):\n            val_pred = self.model.predict(X_valid_pad[:32], verbose=0)  # Sample validation\n            valid_pairs, clashes = check_structure(valid_sequences.iloc[0]['sequence'], \n                                                   val_pred[0][:len(valid_sequences.iloc[0]['sequence'])])\n            logs['val_pairs'] = len(valid_pairs)\n            logs['val_clashes'] = len(clashes)\n            print(f\" | Val_pairs: {len(valid_pairs)} | Val_clashes: {len(clashes)}\")\n\n    print(\"Training student model with physical constraints...\")\n    history_student_model_distill = student_model.fit(\n        student_train_dataset, combined_y_train,\n        validation_data=valid_dataset,\n        epochs=50,\n        callbacks=[EarlyStopping(monitor='val_loss', patience=3), PhysicalConstraintCallback()],\n        verbose=1\n    )\n\n    teacher_model = student_model\n","metadata":{"_uuid":"8b6e7d19-9670-4f06-9995-61f156f5588e","_cell_guid":"fbd23461-6579-49ed-adba-4c414b97a74b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-02-28T00:37:16.089360Z","iopub.execute_input":"2025-02-28T00:37:16.089655Z"}},"outputs":[{"name":"stdout","text":"\nPerforming distillation step 1/3 with physical constraints\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 6. Generating Predictions and Submission File\n\nFor each test sequence, we predict the 3D coordinates using our trained CNN model.\n\nThe submission requires 5 sets of coordinates per target. In this baseline, we replicate the same predicted structure 5 times.","metadata":{"_uuid":"fe88f530-9cd2-4dd5-94f2-41e2021c50f9","_cell_guid":"fe7b78f7-a78f-4cda-b597-5cbf25d98f28","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# 6. Generating Predictions and Verification\nX_test = test_sequences['encoded'].tolist()\nX_test_pad = pad_sequences(X_test, maxlen=max_len, padding='post', value=0)\npredictions = teacher_model.predict(X_test_pad)\n\n# Physical Soundness Verification Functions\ndef is_complementary(base1, base2):\n    pairs = {'A': ['U'], 'U': ['A', 'G'], 'G': ['C', 'U'], 'C': ['G']}\n    return base2 in pairs.get(base1, [])\n\ndef calculate_distance(coord1, coord2):\n    return np.sqrt(sum((a - b)**2 for a, b in zip(coord1, coord2)))\n\ndef check_structure(sequence, coords):\n    valid_pairs = []\n    clashes = []\n    pair_distance_range = (2.5, 4.0)  # Base pairing distance in Å\n    clash_threshold = 2.0\n    \n    for i in range(len(sequence)):\n        for j in range(i+1, len(sequence)):\n            if abs(i - j) < 4: continue\n            \n            distance = calculate_distance(coords[i], coords[j])\n            \n            if is_complementary(sequence[i], sequence[j]):\n                if pair_distance_range[0] <= distance <= pair_distance_range[1]:\n                    valid_pairs.append((i+1, j+1, round(distance, 2)))\n\n            if abs(i - j) > 1 and distance < clash_threshold:\n                clashes.append((i+1, j+1, round(distance, 2)))\n\n    return valid_pairs, clashes\n\n# Verify all test predictions before submission\nprint(\"\\n=== Physical Soundness Verification ===\")\nfor idx, row in test_sequences.iterrows():\n    target_id = row['target_id']\n    seq = row['sequence']\n    pred_coords = predictions[idx][:len(seq)]  # Remove padding\n    \n    valid_pairs, clashes = check_structure(seq, pred_coords)\n    \n    print(f\"\\nTarget: {target_id}\")\n    print(f\"Valid base pairs: {len(valid_pairs)} | Steric clashes: {len(clashes)}\")\n    if clashes:\n        print(f\"WARNING: {len(clashes)} steric clashes detected!\")\n        print(f\"Example clashes (residues, distance): {clashes[:3]}\")","metadata":{"_uuid":"d1e938fc-98b7-499c-b05b-b6edd621c9d0","_cell_guid":"fbab4072-6a39-49e3-b6f7-57e64556e9d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Saving the Submission File\n\nFinally, we save the submission file as `submission.csv`.","metadata":{"_uuid":"7d9bbfd9-5f22-46bb-9a74-86c64c32623e","_cell_guid":"e122805e-51a2-411f-81f8-072ff85e4f5a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# 7. Generate Submission File After Verification\nsubmission_rows = []\nfor idx, row in test_sequences.iterrows():\n    target_id = row['target_id']\n    pred_coords = predictions[idx][:len(row['encoded'])]  # Actual residues\n    \n    for i in range(len(pred_coords)):\n        coords = pred_coords[i]\n        submission_rows.append({\n            'ID': f\"{target_id}_{i+1}\",\n            'resname': row['sequence'][i],\n            'resid': i+1,\n            **{f\"x_{j+1}\": coords[0] for j in range(5)},\n            # ... rest of coordinate columns\n        })\n\nsubmission_df = pd.DataFrame(submission_rows)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Final submission generated with verified predictions\")","metadata":{"_uuid":"2632f190-329c-4373-9632-b03b99d89e90","_cell_guid":"f1b1e66f-6ed2-4fed-b7bc-b5b9d2a306c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}